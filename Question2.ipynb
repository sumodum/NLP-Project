{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim.downloader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trec_train 0: 1162\n",
      "trec_train 1: 1250\n",
      "trec_train 2: 86\n",
      "trec_train 3: 1223\n",
      "trec_train 4: 896\n",
      "trec_train 5: 835\n",
      "     label-coarse  label-fine  \\\n",
      "0               0           0   \n",
      "1               1           1   \n",
      "2               0           0   \n",
      "3               1           2   \n",
      "4          OTHERS           3   \n",
      "...           ...         ...   \n",
      "5447            1          14   \n",
      "5448            1          46   \n",
      "5449            4          41   \n",
      "5450            4          41   \n",
      "5451            1          46   \n",
      "\n",
      "                                                   text  \n",
      "0     How did serfdom develop in and then leave Russ...  \n",
      "1      What films featured the character Popeye Doyle ?  \n",
      "2     How can I find a list of celebrities ' real na...  \n",
      "3     What fowl grabs the spotlight after the Chines...  \n",
      "4                       What is the full form of .com ?  \n",
      "...                                                 ...  \n",
      "5447            What 's the shape of a camel 's spine ?  \n",
      "5448           What type of currency is used in China ?  \n",
      "5449                    What is the temperature today ?  \n",
      "5450              What is the temperature for cooking ?  \n",
      "5451               What currency is used in Australia ?  \n",
      "\n",
      "[4952 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "trec_train = pd.read_csv('./TREC_data/train.csv')\n",
    "trec_test = pd.read_csv('./TREC_data/test.csv')\n",
    "for i in range(6):\n",
    "    print(f'trec_train {i}: {len(trec_train[trec_train[\"label-coarse\"]==i])}')\n",
    "    # print(f'trec_test {i}: {len(trec_test[trec_test[\"label-coarse\"]==i])}')\n",
    "\n",
    "# preprocess datasets\n",
    "trec_train.loc[((trec_train['label-coarse']==2) | (trec_train['label-coarse']==5)), 'label-coarse'] = 'OTHERS'\n",
    "trec_test.loc[((trec_test['label-coarse']==2) | (trec_test['label-coarse']==5)), 'label-coarse'] = 'OTHERS'\n",
    "\n",
    "# split train into train & dev; dev contains 500 unique samples\n",
    "dev = trec_train.sample(n = 500,replace = False)\n",
    "train_new = trec_train.drop(dev.index)\n",
    "\n",
    "print(train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pretrained word embeddings \n",
    "google_news = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "vocab_dict = google_news.key_to_index\n",
    "vocab_dict['<PAD>'] = vocab_dict[list(vocab_dict.keys())[-1]]+1\n",
    "w2v_vec = google_news.vectors\n",
    "w2v_vec = np.append(w2v_vec, [np.zeros(300)], axis=0)\n",
    "print(w2v_vec[vocab_dict['<PAD>']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design neural network using the pretrained word embeddings\n",
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, w2v_vec, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super(QuestionClassifier, self).__init__()\n",
    "\n",
    "        # embed with pretrained weights\n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings = torch.FloatTensor(w2v_vec), freeze = True, padding_idx = vocab_dict['<PAD>'])\n",
    "        \n",
    "        # bidirectional RNN layer (LSTM) -- google_news.vector_size is the input size/embedding dimensions\n",
    "        self.rnn = nn.LSTM(google_news.vector_size, hidden_dim, num_layers = num_layers, bidirectional = bidirectional, dropout = dropout, batch_first=True)\n",
    "        \n",
    "        # fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        # output layer\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        # dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # embedding layer -- text: input sentence/question\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # RNN layer\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        # aggregate by taking the mean/average, max value or sum\n",
    "        # aggregate = torch.mean(outputs, dim=1)\n",
    "        # aggregate, indices = torch.max(outputs, dim=1) \n",
    "        aggregate =  torch.sum(outputs, dim=1)\n",
    "\n",
    "        # fully connected layer\n",
    "        output = self.fc(aggregate)\n",
    "        \n",
    "        # apply softmax to predict probabilities\n",
    "        softmax_output = self.softmax(output)\n",
    "\n",
    "        return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert sentences into tensors\n",
    "def text_to_tensor(text, vocab_dict):\n",
    "    input_text = []\n",
    "    max_len = len((max([word_tokenize(i) for i in text], key=len)))\n",
    "    for i in text:\n",
    "        tokens = word_tokenize(i)\n",
    "        # find indices of tokens from google_news w2v (drop unknown words)\n",
    "        text_idx = [vocab_dict[token] for token in tokens if token in vocab_dict]\n",
    "        # add padding\n",
    "        text_idx = text_idx + [vocab_dict['<PAD>']] * (max_len - len(text_idx))\n",
    "        input_text.append(text_idx)\n",
    "        # print(input_text)\n",
    "    input_text_tensor = torch.LongTensor(input_text)\n",
    "    return input_text_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train and test loops to be used in each epoch\n",
    "def training_loop(model, x_train, y_train, batch_size, optimizer, loss_fn):\n",
    "    running_loss = 0\n",
    "\n",
    "    for size in range(0, len(x_train), batch_size):\n",
    "        # obtain input data and corresponding labels\n",
    "        text, labels = x_train[size:size+batch_size], y_train[size:size+batch_size]\n",
    "        \n",
    "        # convert text and labels into tensors\n",
    "        input_text = text_to_tensor(text, vocab_dict) # using word embedding and aggregation???\n",
    "        # input_labels = torch.as_tensor(labels.to_numpy(dtype=np.long), dtype=torch.long)\n",
    "        input_labels = torch.as_tensor(labels.to_numpy(dtype=np.float64), dtype=torch.long)\n",
    "        \n",
    "        # forward pass\n",
    "        softmax_output = model.forward(input_text)\n",
    "        softmax_output = softmax_output.type(torch.FloatTensor)\n",
    "        # input_labels = input_labels.type(torch.FloatTensor)\n",
    "\n",
    "        loss = loss_fn(softmax_output, input_labels) # CrossEntropy expects float inputs and long labels??\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "    \n",
    "    running_loss = running_loss / len(x_train)\n",
    "        \n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def testing_loop(model, x_test, y_test):\n",
    "    test_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in x_test.index:\n",
    "            input_text = text_to_tensor([x_test[i]], vocab_dict)\n",
    "            # input_labels = torch.as_tensor(y_test[i], dtype=torch.int)\n",
    "            pred = model.forward(input_text)\n",
    "            # print(pred.argmax())\n",
    "            # print(y_test[i])\n",
    "            test_accuracy += (pred.argmax() == y_test[i]).type(torch.float).sum().item()\n",
    "\n",
    "    test_accuracy = float(test_accuracy) / len(x_test)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_acc = -np.inf\n",
    "\n",
    "    def early_stop(self, validation_acc): # min_validation_acc is the basis of comparison (the new max val_accuracy)\n",
    "        if validation_acc > self.min_validation_acc:\n",
    "            self.min_validation_acc = validation_acc\n",
    "            self.counter = 0\n",
    "        elif validation_acc <= (self.min_validation_acc + self.min_delta): # if val_accuracy does not improve for <patience> times, early stopper is enabled\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters (higher hidden_dim, smaller learning_rate, higher no of epochs)\n",
    "hidden_dim = 30 # no neurons in each layer\n",
    "output_dim = 5 # no of classes\n",
    "num_layers = 3\n",
    "bidirectional = True\n",
    "dropout = 0.2\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.0000001\n",
    "\n",
    "# initialise other variables\n",
    "model = QuestionClassifier(w2v_vec, hidden_dim, output_dim, num_layers, bidirectional, dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "early_stopper = EarlyStopper(patience=5)\n",
    "loss_fn = nn.CrossEntropyLoss() # for multiclass classification\n",
    "batch_size = 32\n",
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train 0: 1044\n",
      "y_train 1: 1125\n",
      "y_train 2: 853\n",
      "y_train 3: 1120\n",
      "y_train 4: 810\n",
      "y_dev 0: 118\n",
      "y_dev 1: 125\n",
      "y_dev 2: 68\n",
      "y_dev 3: 103\n",
      "y_dev 4: 86\n"
     ]
    }
   ],
   "source": [
    "# split data into x and y\n",
    "x_train = train_new['text']\n",
    "y_train = train_new['label-coarse']\n",
    "x_dev = dev['text']\n",
    "y_dev = dev['label-coarse']\n",
    "\n",
    "train_small_1 = train_new[:10000]\n",
    "x_train_small = train_small_1['text']\n",
    "y_train_small = train_small_1['label-coarse']\n",
    "dev_small = dev[:3000]\n",
    "x_dev_small = dev_small['text']\n",
    "y_dev_small = dev_small['label-coarse']\n",
    "\n",
    "# train_small_1 = train_new[((train_new['label-coarse']==1) | (train_new['label-coarse']==3))]\n",
    "# x_train_small = train_small_1['text']\n",
    "# y_train_small = train_small_1['label-coarse']\n",
    "# print(len(y_train_small[y_train_small==1]))\n",
    "# dev_small = dev[((dev['label-coarse']==1) | (dev['label-coarse']==3))]\n",
    "# x_dev_small = dev_small['text']\n",
    "# y_dev_small = dev_small['label-coarse']\n",
    "\n",
    "# transform 'others' label to 2\n",
    "y_train_small = y_train_small.replace('OTHERS', 2)\n",
    "y_dev_small = y_dev_small.replace('OTHERS', 2)\n",
    "\n",
    "# transform 'others' label to 2\n",
    "y_train = y_train.replace('OTHERS', 2)\n",
    "y_dev = y_dev.replace('OTHERS', 2)\n",
    "\n",
    "\n",
    "x_test = trec_test['text']\n",
    "y_test = trec_test['label-coarse']\n",
    "y_test = y_test.replace('OTHERS', 2)\n",
    "\n",
    "# getting distribution of data\n",
    "for i in range(5):\n",
    "    print(f'y_train {i}: {len(y_train[y_train==i])}')\n",
    "for i in range(5):\n",
    "    print(f'y_dev {i}: {len(y_dev[y_dev==i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starting...\n",
      "Epoch 1: Train loss = 0.05030913032314504, Validation accuracy = 0.328\n",
      "Epoch 2: Train loss = 0.04932729192138296, Validation accuracy = 0.396\n",
      "Epoch 3: Train loss = 0.04640949460917028, Validation accuracy = 0.494\n",
      "Epoch 4: Train loss = 0.04494996544617636, Validation accuracy = 0.528\n",
      "Epoch 5: Train loss = 0.043582868282359716, Validation accuracy = 0.574\n",
      "Epoch 6: Train loss = 0.04162421762365517, Validation accuracy = 0.614\n",
      "Epoch 7: Train loss = 0.039211541125385364, Validation accuracy = 0.642\n",
      "Epoch 8: Train loss = 0.03771410564709942, Validation accuracy = 0.672\n",
      "Epoch 9: Train loss = 0.03665730023759632, Validation accuracy = 0.686\n",
      "Epoch 10: Train loss = 0.036172728473997655, Validation accuracy = 0.712\n",
      "Epoch 11: Train loss = 0.03565348717286244, Validation accuracy = 0.718\n",
      "Epoch 12: Train loss = 0.0353535564679514, Validation accuracy = 0.736\n",
      "Epoch 13: Train loss = 0.03499431070391697, Validation accuracy = 0.73\n",
      "Epoch 14: Train loss = 0.03478640821041699, Validation accuracy = 0.726\n",
      "Epoch 15: Train loss = 0.034682571165781224, Validation accuracy = 0.754\n",
      "Epoch 16: Train loss = 0.03446615850838783, Validation accuracy = 0.75\n",
      "Epoch 17: Train loss = 0.03431536818166927, Validation accuracy = 0.76\n",
      "Epoch 18: Train loss = 0.034023797401318834, Validation accuracy = 0.746\n",
      "Epoch 19: Train loss = 0.03395828376005077, Validation accuracy = 0.776\n",
      "Epoch 20: Train loss = 0.03390368584525412, Validation accuracy = 0.778\n",
      "Epoch 21: Train loss = 0.03379644759252114, Validation accuracy = 0.756\n",
      "Epoch 22: Train loss = 0.033562045478185275, Validation accuracy = 0.764\n",
      "Epoch 23: Train loss = 0.03349137727490149, Validation accuracy = 0.782\n",
      "Epoch 24: Train loss = 0.033306583027943655, Validation accuracy = 0.782\n",
      "Epoch 25: Train loss = 0.033274597878024725, Validation accuracy = 0.788\n",
      "Epoch 26: Train loss = 0.03304091294526283, Validation accuracy = 0.776\n",
      "Epoch 27: Train loss = 0.033041522123509345, Validation accuracy = 0.788\n",
      "Epoch 28: Train loss = 0.032975311473806376, Validation accuracy = 0.788\n",
      "Epoch 29: Train loss = 0.03295253279809998, Validation accuracy = 0.808\n",
      "Epoch 30: Train loss = 0.032879726386513194, Validation accuracy = 0.792\n",
      "Epoch 31: Train loss = 0.032807757858313344, Validation accuracy = 0.8\n",
      "Epoch 32: Train loss = 0.032803803140685706, Validation accuracy = 0.792\n",
      "Epoch 33: Train loss = 0.03271179765414729, Validation accuracy = 0.792\n",
      "Epoch 34: Train loss = 0.032637747645570696, Validation accuracy = 0.8\n",
      "Early Stopping at 34 epochs...\n",
      "Training finished!\n",
      "Time taken for training with 34 epochs: 146.48157930374146seconds\n",
      "Testing starting...\n",
      "Test accuracy = 0.812\n"
     ]
    }
   ],
   "source": [
    "# file = open(\"./q2_mean.txt\", \"a\")\n",
    "# file = open(\"./q2_max.txt\", \"a\")\n",
    "file = open(\"./q2_sum.txt\", \"a\")\n",
    "print(\"Epoch, Train loss, Validation accuracy\", file=file)\n",
    "\n",
    "import time \n",
    "\n",
    "print(\"Training starting...\")\n",
    "start_t = time.time()\n",
    "accuracy = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = training_loop(model, x_train, y_train, batch_size, optimizer, loss_fn)\n",
    "    val_accuracy = testing_loop(model, x_dev, y_dev)\n",
    "\n",
    "    accuracy.append(val_accuracy)\n",
    "    print(f\"Epoch {epoch+1}: Train loss = {train_loss}, Validation accuracy = {val_accuracy}\")\n",
    "\n",
    "    print(epoch+1, train_loss, val_accuracy, file=file)\n",
    "\n",
    "    if epoch >= 1 and early_stopper.early_stop(val_accuracy): # early stopper is done on test/validation datasets\n",
    "        print(f\"Early Stopping at {epoch+1} epochs...\") \n",
    "        break\n",
    "    \n",
    "\n",
    "end_t = time.time()\n",
    "print(\"Training finished!\")\n",
    "\n",
    "print(\"Time taken for training with {} epochs: {}seconds\".format(epoch+1, end_t - start_t))\n",
    "print(\"Time taken for training with {} epochs: {}seconds\".format(epoch+1, end_t - start_t), file = file)\n",
    "\n",
    "\n",
    "print(\"Testing starting...\")\n",
    "test_accuracy = testing_loop(model, x_test, y_test)\n",
    "print(f\"Test accuracy = {test_accuracy}\")\n",
    "\n",
    "\n",
    "print(f\"Test accuracy = {test_accuracy}\", file=file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For mean\n",
      "\tAverage train loss: 0.040\n",
      "\tMax train loss: 0.050\n",
      "\tAverage validation accuracy: 0.634\n",
      "\tMax validation accuracy: 0.758\n",
      "\tTest accuracy: 0.772\n",
      "\n",
      "For max\n",
      "\tAverage train loss: 0.038\n",
      "\tMax train loss: 0.050\n",
      "\tAverage validation accuracy: 0.647\n",
      "\tMax validation accuracy: 0.758\n",
      "\tTest accuracy: 0.794\n",
      "\n",
      "For sum\n",
      "\tAverage train loss: 0.036\n",
      "\tMax train loss: 0.050\n",
      "\tAverage validation accuracy: 0.710\n",
      "\tMax validation accuracy: 0.808\n",
      "\tTest accuracy: 0.812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare different aggregate methods\n",
    "agg_methods = [\"mean\", \"max\", \"sum\"]\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    with open(\"./q2_{}.txt\".format(agg_methods[i]), \"r\") as file:\n",
    "        lines = file.readlines() \n",
    "        lines_to_read = lines[1:-2] \n",
    "        items = [line.split() for line in lines_to_read]\n",
    "        train_loss = [float(x) for x in (line[1] for line in items)]\n",
    "        avg1 = \"{:.3f}\".format(sum(train_loss) / len(train_loss))\n",
    "        max1 = \"{:.3f}\".format(max(train_loss))\n",
    "        val_accuracy = [float(x) for x in (line[2] for line in items)]\n",
    "        avg2 = \"{:.3f}\".format(sum(val_accuracy) / len(val_accuracy))\n",
    "        max2 = \"{:.3f}\".format(max(val_accuracy))\n",
    "        test_line = lines[-1]\n",
    "        test_acc = test_line.split()[-1]\n",
    "    print(\"For {}\\n\\tAverage train loss: {}\\n\\tMax train loss: {}\\n\\tAverage validation accuracy: {}\\n\\tMax validation accuracy: {}\\n\\tTest accuracy: {}\\n\".format(agg_methods[i], avg1, max1, avg2, max2, test_acc))\n",
    "\n",
    "# so SUM is the best aggregate method...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print out validation accuracies for sum training\n",
    "\n",
    "file_acc = open(\"./q2_sum_val_accuracies.txt\", \"a\")\n",
    "\n",
    "with open(\"./q2_sum.txt\".format(agg_methods[i]), \"r\") as file:\n",
    "    lines = file.readlines() \n",
    "    lines_to_read = lines[1:-2] \n",
    "\n",
    "    for line in lines_to_read:\n",
    "        items = line.split()\n",
    "        epoch = items[0]\n",
    "        val_accuracy = float(items[2])\n",
    "        print(f\"Epoch {epoch}: {val_accuracy}\", file=file_acc)\n",
    "\n",
    "\n",
    "file.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
